{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "208947d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader #To load the pdf document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter #Perform chunking\n",
    "from langchain_huggingface import HuggingFaceEndpoint ,ChatHuggingFace ,HuggingFaceEmbeddings#To initilazie the model\n",
    "from dotenv import load_dotenv #Load environmental variables\n",
    "load_dotenv()\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS #Faiss vectorstore\n",
    "from langchain_core.prompts import PromptTemplate #To create a Instruction prompt\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever #Generate multiple query for user's single query\n",
    "from langchain_groq import ChatGroq #Open source LLM models \n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain_core.messages import HumanMessage, AIMessage, trim_messages #Interaction between human and AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0bb4f",
   "metadata": {},
   "source": [
    "## Step-1: Load the PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5b851a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Intro_about_AI.pdf\" #Path to pdf file\n",
    "loader = PyPDFLoader(file_path) #Load the file\n",
    "docs= []\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d9a74296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs) #Page wise document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c4e26",
   "metadata": {},
   "source": [
    "## Step-2: Perform Chunking using RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bd0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\",\"\\n\",\".\"], #pripority based seperation \n",
    "    chunk_size = 1000,  \n",
    "    chunk_overlap = 120,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "#Apply splitting/chunking\n",
    "chunks = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877ca314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks) #Total number of chunks formed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58430f21",
   "metadata": {},
   "source": [
    "## Step-3: Load the LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873c45a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prabal Kuinkel\\Desktop\\RAG_Based_PDF_Chatbot\\rag_pdf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Loading the embedding model from huggingface\n",
    "embedding_model = HuggingFaceEmbeddings(model_name =\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb71fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loading the llm model from hugging face\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\", #Mistral model\n",
    "#     temperature= 0.4,\n",
    "#     max_new_tokens= 200,  #Maximun number of tokens to be generated in output\n",
    "# )\n",
    "# model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baeff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the groq hosted model\n",
    "model = ChatGroq(model_name = \"Llama-3.3-70b-Versatile\",max_tokens= 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d8aeb",
   "metadata": {},
   "source": [
    "## Step-4: Creating a VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b983b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store...\n",
      "Vector store saved to faiss_index\n"
     ]
    }
   ],
   "source": [
    "VECTOR_STORE_PATH = \"faiss_index\"  # Directory to save the vector store\n",
    "\n",
    "# Check if vector store exists\n",
    "if os.path.exists(VECTOR_STORE_PATH):\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vector_store = FAISS.load_local(VECTOR_STORE_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"Creating new vector store...\")\n",
    "    # creating/storing chunks \n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "    # Saving the vector store locally\n",
    "    vector_store.save_local(VECTOR_STORE_PATH)\n",
    "    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69bc29c",
   "metadata": {},
   "source": [
    "## Step-5: Creating a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template = \"\"\"You are a Smart Chat Assistant tasked with answering the user question , based on the context provided. Only answer based on the provided context .\n",
    "    If the context does not contain enough information to answer, respond: I do not have enough information to answer your question.\n",
    "    Keep answers concise, clear, and directly relevant to the question.\n",
    "    'Context':\n",
    "    {context}\n",
    "    'Question':\n",
    "    {input}\"\"\",\n",
    "    input_variables=['context','input']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e433da",
   "metadata": {},
   "source": [
    "## Step-6: Creating a retriever component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636df7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi query retriever takes 2 arguemnts : (\"which llm to use\",\"which retriever to use\")\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_store.as_retriever\n",
    "    (search_kwargs={\"k\": 6,'lambda_mult':0.3},  #'k' : number of similari documents to retrieve , 'lambda_mult': to retriever the diverse documents and reduce redundancy\n",
    "    search_type=\"mmr\"), #Maximum marginal relevance\n",
    "    llm=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d512ad0",
   "metadata": {},
   "source": [
    "## Step-7: Creating a RAG_Chain / Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel , RunnablePassthrough\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'context' :multiquery_retriever, #Retrieve context from vectorstore for user query\n",
    "    'input' :RunnablePassthrough() #No processing, Just pass what received\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04eae586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final rag chain , conbining : retreivers , template , model and output parser\n",
    "rag_chain = parallel_chain | prompt_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the Human and AI messages\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2cbdb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question asked by user\n",
    "query = \"what are the 2 questions ?\"\n",
    "messages.append(HumanMessage(content=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95c98e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is this pdf about ?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='This PDF appears to be an introduction to Artificial Intelligence (AI), covering topics such as the definition of AI, its benefits, examples of AI applications, and the future of AI, including its potential risks and regulations.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='can you further elaborate it ?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The PDF discusses the introduction to Artificial Intelligence (AI), covering various aspects such as its definition, benefits, examples, and future implications. \\n\\nIt starts by defining AI as computer systems that can perform tasks associated with human cognitive functions, such as interpreting speech, playing games, and identifying patterns. The PDF also explains how AI systems learn by processing large amounts of data and looking for patterns.\\n\\nThe benefits of AI mentioned in the PDF include automating repetitive tasks, solving complex problems, reducing human error, and improving customer experience. It also highlights the application of AI in various fields, including healthcare, transportation, and education.\\n\\nThe PDF further discusses the examples of AI, such as generative AI tools, smart assistants, self-driving cars, wearables, and visual filters. It also touches upon the topic of AI regulation and the future of AI, including the potential risks and challenges associated with its development and deployment.\\n\\nOverall, the PDF provides a comprehensive overview of AI, its capabilities, applications, and implications, serving as a useful introduction to the subject.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='How many questions have i asked you till now ?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='You have asked 2 questions till now.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what are the 2 questions ?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1f45f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only store past 3 conversation due to token limit\n",
    "selected_messages = trim_messages(\n",
    "    messages,\n",
    "    token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
    "    max_tokens=6,  # <-- allow up to past 6 messages (equivalent to 3 conversations).\n",
    "    strategy=\"last\",\n",
    "    start_on=\"human\",\n",
    "    \n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n",
    "result = rag_chain.invoke(selected_messages)\n",
    "messages.append(AIMessage(content=result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0891a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is this pdf about ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "This PDF appears to be an introduction to Artificial Intelligence (AI), covering topics such as the definition of AI, its benefits, examples of AI applications, and the future of AI, including its potential risks and regulations.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "can you further elaborate it ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The PDF discusses the introduction to Artificial Intelligence (AI), covering various aspects such as its definition, benefits, examples, and future implications. \n",
      "\n",
      "It starts by defining AI as computer systems that can perform tasks associated with human cognitive functions, such as interpreting speech, playing games, and identifying patterns. The PDF also explains how AI systems learn by processing large amounts of data and looking for patterns.\n",
      "\n",
      "The benefits of AI mentioned in the PDF include automating repetitive tasks, solving complex problems, reducing human error, and improving customer experience. It also highlights the application of AI in various fields, including healthcare, transportation, and education.\n",
      "\n",
      "The PDF further discusses the examples of AI, such as generative AI tools, smart assistants, self-driving cars, wearables, and visual filters. It also touches upon the topic of AI regulation and the future of AI, including the potential risks and challenges associated with its development and deployment.\n",
      "\n",
      "Overall, the PDF provides a comprehensive overview of AI, its capabilities, applications, and implications, serving as a useful introduction to the subject.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How many questions have i asked you till now ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You have asked 2 questions till now.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what are the 2 questions ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The two questions you have asked me till now are: \n",
      "1. \"can you further elaborate it ?\"\n",
      "2. \"How many questions have i asked you till now ?\" \n",
      "3. \"what are the 2 questions ?\" \n",
      "\n",
      "So actually you have asked 3 questions.\n"
     ]
    }
   ],
   "source": [
    "for msg in messages:\n",
    "    msg.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
